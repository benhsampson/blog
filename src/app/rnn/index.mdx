import { Container } from "../Container.tsx";
import SplitContainer from "../SplitContainer.tsx";
import SectionLink from "./SectionLink.tsx";

<Container>

# Notes on: The Unreasonable Effectiveness of Recurrent Neural Networks

These are some of my notes from Andrej Karpathy's [The Unreasonable Effectiveness of Recurrent Neural Networks](http://karpathy.github.io/2015/05/21/rnn-effectiveness/).
I've also done a mathematical breakdown of the [Github gist](https://gist.github.com/karpathy/d4dee566867f8291f086) he posted that implements a RNN using only NumPy.

## RNNs

A glaring limitation of Convolutional Neural Nets is that their API is too constrained: they accept a fixed-sized vector as input (e.g. an image) and produce a fixed-sized vector (e.g. probabilities of different classes) as output.
These models perform these mappings in a fixed number of computational steps (e.g. the number of layers in the model).
The core reason that RNNs are more exciting is they allow for _sequences_ in the input and output.
The choice of using sequences as input/output can depend on the application, e.g.:

- Image classification - fixed to fixed
- Image captioning - sequence output
- Sentiment analysis - sequence input
- Machine translation - sequence input and output
- Video classification (where we wish to label every frame of the video) - synced sequence input and output

RNNs combine the input with their state vector to produce a new state vector after undergoing a static function called the _recurrent transformation_.
Note: though the inputs to this function change, the function _itself_ is static.
Note: There is no predetermined limit on the length of sequences since the recurrent transformation can be applied as many times as like.
This sequenced regime is much more powerful than fixed networks that are doomed from the get-go with their fixed number of computational steps.

It doesn't have to be that your inputs/outputs lend themselves naturally to being represented as sequences.
Image classification can be done sequentially by steering its attention around an image.[^1]
[^1]: [Multiple Object Recognition with Visual Attention](https://arxiv.org/abs/1412.7755)
Images of digits have been created by sequentially adding color to a canvas.[^2]
[^2]: [DRAW: A Recurrent Neural Network For Image Generation](https://arxiv.org/abs/1502.04623)
Even though in both these cases, data is not in the form of sequences; we can process it sequentially and benefit from the stateful processing given to us from RNNs.

**RNN computation.**
An RNN's forward pass accepts and input vector $$\vec{x}$$ and produce an output vector $$\vec{y}$$.
However instead of the $$\vec{y}=W\vec{x}+\vec{b}$$ you may be used to, we have:

$$
\begin{align}
\vec{h^t} = \tanh(W_{hh}\vec{h^{t-1}}+W_{xh}\vec{x^{t}}+\vec{b_h})\\
\vec{y^t} = W_{hy}\vec{h^t}+\vec{b_y}
\end{align}
$$

for the $$t$$'th element in the sequence.
The crucial difference is that an RNN's output $$\vec{y^t}$$ is not only a product of the input vector put in $$\vec{x^t}$$, but also the entire history of inputs you've fed in the past, which manifests as the previous element's hidden state vector $$\vec{h^{t-1}}$$.
So our model has five parameters: $$W_{hh}, W_{xh}, W_{hy}, \vec{b_h}, \vec{b_y}$$.
The element-wise $$\tanh$$ squashes the activations to the range $[-1, 1]$ and adds that crucial bit of nonlinearity.

Note: the Long Short-Term Memory (LSTM) works a little better in practice, owing to it's more powerful update equation and backprop dynamics.

</Container>
<CH.Section>
<SplitContainer>
<SplitContainer.Text>

## Our network

Our char-level RNN is going to try and predict the next char e.g. predict 'e' given "h", 'l' given "he", 'l' given "hel", and 'o' given "hell", predicting "hello";
We're going to use categorical cross-entropy for our loss function and AdaGrad to optimize the parameters.

### Maths

#### Softmax cross-entropy loss

We take the "softmax" ($$\vec{\sigma}$$) of our output vector $$\vec{y^t}$$ to squash it such that all the elements in $$\vec{\sigma}(\vec{y^t})$$ add up to 1:

<SectionLink focus="42" id="focus://42">

$$
\sigma(\vec{y^t})_i=\frac{e^{y^t_i}}{\sum_{j=1}^{K}e^{y^t_j}}
$$

</SectionLink>

where $$K$$ is the number of classes.
In our network, there is only one correct answer to the question: what is the next char in the sequence?
I.e. the labels are one-hot.
Hence the formula for cross-entropy loss is:

<SectionLink focus="43" id="focus://43">

$$
L^t=-\log({\sigma(\vec{y^t})_c})
$$

</SectionLink>

where $$c$$ is the label of the correct class.

#### Backprop

Our goal with backprop is to find the 1st-order derivatives of our loss $$L^t$$ with respect to our model parameters.
So let's start with finding the derivative of our loss with respect to the output $$\vec{y^t}$$ and work our way back.
For brevity: if not specified, all variables are for the $$t$$'th element in the sequence.

For $$i=c$$:

<SectionLink focus="49:50" id="focus://49:50">

$$
\begin{align}
\frac{\partial{L}}{\partial{y_i}}=\frac{\partial{L}}{\partial{y_c}}&=\frac{\partial{L}}{\partial{\sigma(\vec{y})_c}}\frac{\partial{\sigma(\vec{y})_c}}{\partial{y_c}}\\
&=(-\frac{1}{\sigma(\vec{y})_c})(\frac{e^{y_c}}{\sum_{j=1}^{K}e^{y_j}}-(\frac{e^{y_c}}{\sum_{j=1}^{K}e^{y_j}})^2)\\
&=\sigma(\vec{y})_c-1
\end{align}
$$

</SectionLink>

by chain and quotient rule.

For $$i\ne c$$:

<SectionLink focus="49" id="focus://49">

$$
\begin{align}
\frac{\partial{L}}{\partial{y_i}}&=\frac{\partial{L}}{\partial{\sigma(\vec{y})_c}}\frac{\partial{\sigma(\vec{y})_c}}{\partial{y_i}}\\
&=(-\frac{1}{\sigma(\vec{y})_c})(-(\frac{e^{y_c}}{\sum_{j=1}^{K}e^{y_j}})^2)\\
&=\sigma(\vec{y})_c
\end{align}
$$

</SectionLink>

by chain and quotient rule again. For $$W_{hy}$$ and $$b_y$$:

<SectionLink focus="51" id="focus://51">

$$
\begin{align}
\frac{\partial{L}}{\partial{W_{hy}}}&=\frac{\partial{L}}{\partial{\vec{y}}}\frac{\partial{\vec{y}}}{\partial{W_{hy}}}\\
&=\frac{\partial{L}}{\partial{\vec{y}}}\vec{h}^T
\end{align}
$$

</SectionLink>

<SectionLink focus="52" id="focus://52">

$$
\begin{align}
\frac{\partial{L}}{\partial{\vec{b_y}}}&=\frac{\partial{L}}{\partial{\vec{y}}}\frac{\partial{\vec{y}}}{\partial{\vec{b_y}}}\\
&=\frac{\partial{L}}{\partial{\vec{y}}}
\end{align}
$$

</SectionLink>

by chain rule.
The hidden layer $$\vec{h^t}$$ is an input of both $$\vec{y^t}$$ and $$\vec{h^{t+1}}$$.
Thus, by the multivariate chain rule:

<SectionLink focus="53:54,58" id="focus://53:54,58">

$$
\begin{align}
\frac{\partial{L^t}}{\partial{\vec{h^t}}}&=\frac{\partial{L^t}}{\partial{\vec{y^t}}}\frac{\partial{\vec{y^t}}}{\partial{\vec{h^t}}}+\frac{\partial{L^t}}{\partial{\vec{h^{t+1}}}}\frac{\partial{\vec{h^{t+1}}}}{\partial{\vec{h^t}}}\\
\end{align}
$$

</SectionLink>

Computing term by term:

<SectionLink focus="53:54,58" id="focus://53:54,58">

$$
\begin{align}
\frac{\partial{L^t}}{\partial{\vec{y^t}}}\frac{\partial{\vec{y^t}}}{\partial{\vec{h^t}}}&=W_{hy}^T\frac{\partial{L^t}}{\partial{\vec{y^t}}}\\
\frac{\partial{\vec{h^{t+1}}}}{\partial{\vec{h^t}}}&=\frac{\partial}{\partial{\vec{h^t}}}(\vec{h^{t+1}}=\tanh(W_{hh}\vec{h^{t}}+W_{xh}\vec{x^{t+1}}+\vec{b_h}))\\
&=W_{hh}^T(1-\vec{h^{t+1}}^2)\odot\frac{\partial{L^t}}{\partial{\vec{h^{t+1}}}}
\end{align}
$$

</SectionLink>

by chain rule and using $$\frac{\partial{\tanh(\vec{z})}}{\partial{\vec{z}}}=1-\tanh^2(\vec{z})$$.
Note that $$\odot$$ is the element-wise product.
For $$\vec{b_h}$$, $$W_{xh}$$, and $$W_{hh}$$:

<SectionLink focus="54:55" id="focus://54:55">

$$
\begin{align}
\frac{\partial{L}}{\partial{\vec{b_h}}}=(1-\vec{h}^2)\frac{\partial{L}}{\partial{\vec{h}}}\\
\end{align}
$$

</SectionLink>

<SectionLink focus="54,56" id="focus://54,56">

$$
\begin{align}
\frac{\partial{L}}{\partial{W_{xh}}}=(1-\vec{h}^2)\frac{\partial{L}}{\partial{\vec{h}}}\vec{x}^T\\
\end{align}
$$

</SectionLink>

<SectionLink focus="54,57" id="focus://54,57">

$$
\begin{align}
\frac{\partial{L^t}}{\partial{W_{hh}}}=(1-\vec{h^t}^2)\frac{\partial{L}}{\partial{h_t}}\vec{h^{t-1}}^T
\end{align}
$$

</SectionLink>

using chain rule and $$\tanh$$ property again.

We've successfully derived the loss derivatives for all five parameters!

</SplitContainer.Text>
<SplitContainer.Code>

```py
"""
Minimal character-level Vanilla RNN model. Written by Andrej Karpathy (@karpathy)
BSD License
"""
import numpy as np

# data I/O
data = open('input.txt', 'r').read() # should be simple plain text file
chars = list(set(data))
data_size, vocab_size = len(data), len(chars)
print 'data has %d characters, %d unique.' % (data_size, vocab_size)
char_to_ix = { ch:i for i,ch in enumerate(chars) }
ix_to_char = { i:ch for i,ch in enumerate(chars) }

# hyperparameters
hidden_size = 100 # size of hidden layer of neurons
seq_length = 25 # number of steps to unroll the RNN for
learning_rate = 1e-1

# model parameters
Wxh = np.random.randn(hidden_size, vocab_size)*0.01 # input to hidden
Whh = np.random.randn(hidden_size, hidden_size)*0.01 # hidden to hidden
Why = np.random.randn(vocab_size, hidden_size)*0.01 # hidden to output
bh = np.zeros((hidden_size, 1)) # hidden bias
by = np.zeros((vocab_size, 1)) # output bias

def lossFun(inputs, targets, hprev):
  """
  inputs,targets are both list of integers.
  hprev is Hx1 array of initial hidden state
  returns the loss, gradients on model parameters, and last hidden state
  """
  xs, hs, ys, ps = {}, {}, {}, {}
  hs[-1] = np.copy(hprev)
  loss = 0
  # forward pass
  for t in xrange(len(inputs)):
    xs[t] = np.zeros((vocab_size,1)) # encode in 1-of-k representation
    xs[t][inputs[t]] = 1
    hs[t] = np.tanh(np.dot(Wxh, xs[t]) + np.dot(Whh, hs[t-1]) + bh) # hidden state
    ys[t] = np.dot(Why, hs[t]) + by # unnormalized log probabilities for next chars
    ps[t] = np.exp(ys[t]) / np.sum(np.exp(ys[t])) # probabilities for next chars
    loss += -np.log(ps[t][targets[t],0]) # softmax (cross-entropy loss)
  # backward pass: compute gradients going backwards
  dWxh, dWhh, dWhy = np.zeros_like(Wxh), np.zeros_like(Whh), np.zeros_like(Why)
  dbh, dby = np.zeros_like(bh), np.zeros_like(by)
  dhnext = np.zeros_like(hs[0])
  for t in reversed(xrange(len(inputs))):
    dy = np.copy(ps[t])
    dy[targets[t]] -= 1 # backprop into y. see http://cs231n.github.io/neural-networks-case-study/#grad if confused here
    dWhy += np.dot(dy, hs[t].T)
    dby += dy
    dh = np.dot(Why.T, dy) + dhnext # backprop into h
    dhraw = (1 - hs[t] * hs[t]) * dh # backprop through tanh nonlinearity
    dbh += dhraw
    dWxh += np.dot(dhraw, xs[t].T)
    dWhh += np.dot(dhraw, hs[t-1].T)
    dhnext = np.dot(Whh.T, dhraw)
  for dparam in [dWxh, dWhh, dWhy, dbh, dby]:
    np.clip(dparam, -5, 5, out=dparam) # clip to mitigate exploding gradients
  return loss, dWxh, dWhh, dWhy, dbh, dby, hs[len(inputs)-1]

def sample(h, seed_ix, n):
  """
  sample a sequence of integers from the model
  h is memory state, seed_ix is seed letter for first time step
  """
  x = np.zeros((vocab_size, 1))
  x[seed_ix] = 1
  ixes = []
  for t in xrange(n):
    h = np.tanh(np.dot(Wxh, x) + np.dot(Whh, h) + bh)
    y = np.dot(Why, h) + by
    p = np.exp(y) / np.sum(np.exp(y))
    ix = np.random.choice(range(vocab_size), p=p.ravel())
    x = np.zeros((vocab_size, 1))
    x[ix] = 1
    ixes.append(ix)
  return ixes

n, p = 0, 0
mWxh, mWhh, mWhy = np.zeros_like(Wxh), np.zeros_like(Whh), np.zeros_like(Why)
mbh, mby = np.zeros_like(bh), np.zeros_like(by) # memory variables for Adagrad
smooth_loss = -np.log(1.0/vocab_size)*seq_length # loss at iteration 0
while True:
  # prepare inputs (we're sweeping from left to right in steps seq_length long)
  if p+seq_length+1 >= len(data) or n == 0:
    hprev = np.zeros((hidden_size,1)) # reset RNN memory
    p = 0 # go from start of data
  inputs = [char_to_ix[ch] for ch in data[p:p+seq_length]]
  targets = [char_to_ix[ch] for ch in data[p+1:p+seq_length+1]]

  # sample from the model now and then
  if n % 100 == 0:
    sample_ix = sample(hprev, inputs[0], 200)
    txt = ''.join(ix_to_char[ix] for ix in sample_ix)
    print '----\n %s \n----' % (txt, )

  # forward seq_length characters through the net and fetch gradient
  loss, dWxh, dWhh, dWhy, dbh, dby, hprev = lossFun(inputs, targets, hprev)
  smooth_loss = smooth_loss * 0.999 + loss * 0.001
  if n % 100 == 0: print 'iter %d, loss: %f' % (n, smooth_loss) # print progress

  # perform parameter update with Adagrad
  for param, dparam, mem in zip([Wxh, Whh, Why, bh, by],
                                [dWxh, dWhh, dWhy, dbh, dby],
                                [mWxh, mWhh, mWhy, mbh, mby]):
    mem += dparam * dparam
    param += -learning_rate * dparam / np.sqrt(mem + 1e-8) # adagrad update

  p += seq_length # move data pointer
  n += 1 # iteration counter
```

</SplitContainer.Code>
</SplitContainer>
</CH.Section>
